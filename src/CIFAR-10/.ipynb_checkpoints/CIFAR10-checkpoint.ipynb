{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica: Redes convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%connect_info\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar 10 y redes convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura que deberas implementar debe tener las siguientes caracteristicas:\n",
    "\n",
    "    - Una capa convolucional de entrada con un filtro de 5x5x3 y 64 filtros de salida\n",
    "    - Una capa de MaxPooling \n",
    "    - Otra capa convolucional con un filtro de 5x5x64 y 64 filtros de salida\n",
    "    - Una capa de MaxPooling\n",
    "    - Una capa completamente conectada con 384 neuronas,cuya activacion es ReLu\n",
    "    - Una capa completamente conectada con 192 neuronas,cuya activacion es ReLu\n",
    "    -Finalmente,una capa de salida con 10 neuronas\n",
    "Es importante señalar que entre cada capa convolucional y de pooling existe una activacion **ReLu**\n",
    "\n",
    "Graficamente, algo similar a lo mostrado en la siguiente figura:\n",
    "<img src=\"AlexNet-cifar10.png\" alt=\"Arquitectura de la red\" title=\"Red convolucional\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar 10\n",
    "Es un conjunto de datos de imagenes que tiene 60,000 imagenes de 30x30 a color de 10 clases distintas (cada clase con 6,000 imagenes).\n",
    "Las clases de este conjunto de datos corresponden a:\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, y truck.\n",
    "\n",
    "Para clasificar este conjunto de datos usaremos una red convolucional anteriormente descrita en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos de este conjunto en las variables correspondientes al igual que el caso de MNIST\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CIFAR_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''Constructor de la red neuronal\n",
    "        En esta funcion defines las operaciones que seran realizadas y \n",
    "        las agregas a self para tenerlas disponibles despues'''\n",
    "        super(CIFAR_NET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        #Toma en cuenta el constructor de las capas convolucionales\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,\n",
    "        #                   padding=0, dilation=1, groups=1, bias=True)\n",
    "        #Una capa de neuronas  se declara con:\n",
    "        #nn.Linear(in_features= x ,out_features=y)\n",
    "#        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Ejecuta forward sobre los datos de entrada x\n",
    "        En esta funcion debes definir de manera consecutiva el orden\n",
    "        de las operaciones capa por capa que definiste en el constructor y\n",
    "        devolver el resultado'''\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self,epochs,data_loader,criterion,optimizer,cuda=False):\n",
    "        '''Entrena la red net, por un numero de epocas \"epochs\" con el data_loader\n",
    "        proporcionado,usando como funcion de perdida la definida en \"criterion\" y el\n",
    "        optimizador pasado como parametro'''\n",
    "        #Primero itera sobre el numero de epocas de entrenamiento    \n",
    "        #Despues,para cada epoca de entrenamiento recorre los datos del dataloader\n",
    "        #Recuerda incluir una manera de monitorizar el desempeño actual de la red\n",
    "        #Es util mostrar el valor actual de perdida y de precision\n",
    "        for epoch in range(2):  \n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Entrenamiento Completo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.152\n",
      "[1,  2000] loss: 1.148\n",
      "[1,  3000] loss: 1.081\n",
      "[1,  4000] loss: 1.020\n",
      "[1,  5000] loss: 0.991\n",
      "[1,  6000] loss: 0.953\n",
      "[1,  7000] loss: 0.929\n",
      "[1,  8000] loss: 0.891\n",
      "[1,  9000] loss: 0.873\n",
      "[1, 10000] loss: 0.842\n",
      "[1, 11000] loss: 0.817\n",
      "[1, 12000] loss: 0.801\n",
      "[2,  1000] loss: 0.778\n",
      "[2,  2000] loss: 0.756\n",
      "[2,  3000] loss: 0.752\n",
      "[2,  4000] loss: 0.750\n",
      "[2,  5000] loss: 0.730\n",
      "[2,  6000] loss: 0.725\n",
      "[2,  7000] loss: 0.731\n",
      "[2,  8000] loss: 0.729\n",
      "[2,  9000] loss: 0.693\n",
      "[2, 10000] loss: 0.705\n",
      "[2, 11000] loss: 0.705\n",
      "[2, 12000] loss: 0.695\n",
      "Entrenamiento Completo\n"
     ]
    }
   ],
   "source": [
    "RedCIFAR=CIFAR_NET()\n",
    "#RedCIFAR.cuda() #puedes descomentar esta linea si tienes GPU disponible\n",
    "#Define el criterio que usaras para evaluar a la red y un optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(RedCIFAR.parameters(), lr=0.001, momentum=0.9)\n",
    "#Entrenamos la red durante 50 pasos(o los que consideres necesarios),con entropia cruzada y el optimizador \n",
    "RedCIFAR.train(50,trainloader,criterion,optimizer,cuda=False) #puedes agregar cuda=True si tienes GPU disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision en conjunto de entrenamiento: 58.8700%\n",
      "Precision en conjunto de validacion: 55.7500%\n"
     ]
    }
   ],
   "source": [
    "prec_train =calcularPrecisionGlobal(RedCIFAR,trainloader,4)\n",
    "prec_val   =calcularPrecisionGlobal(RedCIFAR,testloader,4)\n",
    "print(\"Precision en conjunto de entrenamiento: %.4f%%\"%(prec_train) )#,cuda=False)\n",
    "print(\"Precision en conjunto de validacion: %.4f%%\"%(prec_val)) #,cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salidas:   ship  ship truck  ship\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "#imshow(torchvision.utils.make_grid(images))\n",
    "#print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "outputs = RedCIFAR(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Salidas: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                            for j in range(4)))\n",
    "                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aciertos en la clase plane : 59 %\n",
      "Aciertos en la clase   car : 63 %\n",
      "Aciertos en la clase  bird : 32 %\n",
      "Aciertos en la clase   cat : 28 %\n",
      "Aciertos en la clase  deer : 62 %\n",
      "Aciertos en la clase   dog : 40 %\n",
      "Aciertos en la clase  frog : 68 %\n",
      "Aciertos en la clase horse : 60 %\n",
      "Aciertos en la clase  ship : 86 %\n",
      "Aciertos en la clase truck : 55 %\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "def matrizConfusion(data, truth):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in truth:\n",
    "            images, labels = data\n",
    "            outputs =RedCIFAR(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    for i in range(10):\n",
    "        print('Aciertos en la clase %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "matrizConfusion(images,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.545\n",
      "[1,  2000] loss: 0.547\n",
      "[1,  3000] loss: 0.567\n",
      "[1,  4000] loss: 0.541\n",
      "[1,  5000] loss: 0.552\n",
      "[1,  6000] loss: 0.548\n",
      "[1,  7000] loss: 0.552\n",
      "[1,  8000] loss: 0.542\n",
      "[1,  9000] loss: 0.544\n",
      "[1, 10000] loss: 0.556\n",
      "[1, 11000] loss: 0.556\n",
      "[1, 12000] loss: 0.554\n",
      "[2,  1000] loss: 0.515\n",
      "[2,  2000] loss: 0.506\n",
      "[2,  3000] loss: 0.528\n",
      "[2,  4000] loss: 0.513\n",
      "[2,  5000] loss: 0.509\n",
      "[2,  6000] loss: 0.510\n",
      "[2,  7000] loss: 0.516\n",
      "[2,  8000] loss: 0.514\n",
      "[2,  9000] loss: 0.524\n",
      "[2, 10000] loss: 0.518\n",
      "[2, 11000] loss: 0.532\n",
      "[2, 12000] loss: 0.523\n",
      "Entrenamiento Completo\n",
      "Aciertos en la clase plane : 70 %\n",
      "Aciertos en la clase   car : 68 %\n",
      "Aciertos en la clase  bird : 49 %\n",
      "Aciertos en la clase   cat : 29 %\n",
      "Aciertos en la clase  deer : 58 %\n",
      "Aciertos en la clase   dog : 45 %\n",
      "Aciertos en la clase  frog : 67 %\n",
      "Aciertos en la clase horse : 63 %\n",
      "Aciertos en la clase  ship : 79 %\n",
      "Aciertos en la clase truck : 69 %\n"
     ]
    }
   ],
   "source": [
    "RedCIFAR.train(50,trainloader,criterion,optimizer,cuda=False) \n",
    "matrizConfusion(images,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane  bird   cat  bird\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def muestraIMG(img):\n",
    "    img = img / 2 + 0.5    \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "muestraIMG(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel1=RedCIFAR.conv1.weight.data\n",
    "kernel2=RedCIFAR.conv2.weight[5,5].data\n",
    "\n",
    "muestraIMG(torchvision.utils.make_grid(kernel1))\n",
    "#muestraIMG(torchvision.utils.make_grid(kernel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer.step()\n",
    "muestraIMG(torchvision.utils.make_grid(kernel1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "muestraIMG(torchvision.utils.make_grid(kernel2))\n",
    "print(kernel2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
